import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import re  # í–‰ì •ë™ íŒŒì‹±ìš© ì •ê·œì‹
import pydeck as pdk

import requests  # ì§€ì˜¤ì½”ë”©ìš©
import time  #ì§€ì˜¤ì½”ë”© í˜¸ì¶œ ê°„ ê°„ê²© ì¡°ì ˆìš©
# ------------------------------------------------------------
# ê¸°ë³¸ ì„¤ì •
# ------------------------------------------------------------
st.set_page_config(
    page_title="CSC - í‰íƒì‹œ ëŒ€ê¸°ì§ˆ ë¦¬ìŠ¤í¬ & ë…¸ì¸ë³µì§€ì‹œì„¤ ë¶„ì„",
    layout="wide"
)

# ì‚¬ìš©ì PC/í´ë¼ìš°ë“œ ê¸°ì¤€ ë°ì´í„° í´ë”
BASE_DIR = (Path(__file__).parent / "data").resolve()

# ëŒ€ê¸°ì˜¤ì—¼ í•­ëª©ë³„ ì»¬ëŸ¼ëª… ë§¤í•‘
POLLUTANT_COLS = {
    "SO2": "ì´ì‚°í™”í™©ì¸¡ì •ê°’(ppm)",
    "NO2": "ì´ì‚°í™”ì§ˆì†Œì¸¡ì •ê°’(ppm)",
    "CO": "ì¼ì‚°í™”íƒ„ì†Œì¸¡ì •ê°’(ppm)",
    "O3": "ì˜¤ì¡´ì¸¡ì •ê°’(ppm)",
    "PM10": "PM10ì¸¡ì •ê°’(ã/ã¥)",
    "PM2.5": "PM25ì¸¡ì •ê°’(ã/ã¥)",
}

POLLUTANT_LABELS = {
    "SO2": "SOâ‚‚(ì´ì‚°í™”í™©)",
    "NO2": "NOâ‚‚(ì´ì‚°í™”ì§ˆì†Œ)",
    "CO": "CO(ì¼ì‚°í™”íƒ„ì†Œ)",
    "O3": "Oâ‚ƒ(ì˜¤ì¡´)",
    "PM10": "PM10(ë¯¸ì„¸ë¨¼ì§€)",
    "PM2.5": "PM2.5(ì´ˆë¯¸ì„¸ë¨¼ì§€)",
}

GRADE_TO_SCORE = {"ì¢‹ìŒ": 1, "ë³´í†µ": 2, "ë‚˜ì¨": 3, "ë§¤ìš°ë‚˜ì¨": 4}

# í‰íƒì‹œ ë²•ì •ë™ 23ê°œ (ë¹„ì „1Â·2ë™, ì‹ ì¥1Â·2ë™ í†µí•©)
LEGAL_EMD = [
    "íŒ½ì„±ì", "ì•ˆì¤‘ì", "í¬ìŠ¹ì", "ì²­ë¶ì",
    "ì§„ìœ„ë©´", "ì„œíƒ„ë©´", "ê³ ë•ë©´", "ì˜¤ì„±ë©´", "í˜„ë•ë©´",
    "ì¤‘ì•™ë™",
    "ì„œì •ë™", "ì†¡íƒ„ë™", "ì§€ì‚°ë™", "ì†¡ë¶ë™",
    "ì‹ ì¥ë™",   # ì‹ ì¥1Â·2ë™ í¬í•¨
    "ì‹ í‰ë™", "ì›í‰ë™", "í†µë³µë™",
    "ë¹„ì „ë™",   # ë¹„ì „1Â·2ë™ í¬í•¨
    "ì„¸êµë™", "ìš©ì´ë™", "ë™ì‚­ë™", "ê³ ë•ë™",
]

# ë™ ì´ë¦„ ë§¤í•‘ (ë¹„ì „1Â·2ë™ â†’ ë¹„ì „ë™, ì‹ ì¥1Â·2ë™ â†’ ì‹ ì¥ë™)
EMD_ALIAS_MAP = {
    # ë¹„ì „ë™ ê³„ì—´
    "ë¹„ì „ë™": "ë¹„ì „ë™",
    "ë¹„ì „1ë™": "ë¹„ì „ë™",
    "ë¹„ì „ 1ë™": "ë¹„ì „ë™",
    "ë¹„ì „2ë™": "ë¹„ì „ë™",
    "ë¹„ì „ 2ë™": "ë¹„ì „ë™",
    # ì‹ ì¥ë™ ê³„ì—´
    "ì‹ ì¥ë™": "ì‹ ì¥ë™",
    "ì‹ ì¥1ë™": "ì‹ ì¥ë™",
    "ì‹ ì¥ 1ë™": "ì‹ ì¥ë™",
    "ì‹ ì¥2ë™": "ì‹ ì¥ë™",
    "ì‹ ì¥ 2ë™": "ì‹ ì¥ë™",
}

# ------------------------------------------------------------
# ìœ í‹¸ í•¨ìˆ˜
# ------------------------------------------------------------
@st.cache_data
def read_csv_safely(path: Path) -> pd.DataFrame:
    """ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ë§ì¶°ì„œ CSV ì½ê¸°."""
    for enc in ("utf-8-sig", "utf-8", "cp949"):
        try:
            return pd.read_csv(path, encoding=enc)
        except UnicodeDecodeError:
            continue
    return pd.read_csv(path, encoding="utf-8", errors="ignore")


@st.cache_data
def load_data():
    """í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ë°ì´í„° í•œ ë²ˆì— ë¡œë“œ."""
    air = read_csv_safely(BASE_DIR / "ê²½ê¸°ë„ëŒ€ê¸°í™˜ê²½ì •ë³´ì›”í‰ê· ìë£Œ.csv")
    grade = read_csv_safely(BASE_DIR / "ê²½ê¸°ë„_ëŒ€ê¸°í™˜ê²½ì •ë³´í•­ëª©ë³„ì§€ìˆ˜ë“±ê¸‰.csv")
    region = read_csv_safely(BASE_DIR / "ê²½ê¸°ë„_ëŒ€ê¸°í™˜ê²½_ì§„ë‹¨í‰ê°€ì‹œìŠ¤í…œ_ì§€ì—­ì •ë³´.csv")
    elderly = read_csv_safely(BASE_DIR / "ê²½ê¸°ë„_í‰íƒì‹œ_ë…¸ì¸ë³µì§€ì‹œì„¤_20250129_(1).csv")
    chem = read_csv_safely(BASE_DIR / "ê²½ê¸°ë„_í‰íƒì‹œ_ìœ í•´í™”í•™ë¬¼ì§ˆ_ì·¨ê¸‰ì‚¬ì—…ì¥_í˜„í™©_20250207.csv")
    cai = read_csv_safely(BASE_DIR / "pyeongtaek_CAI_index.csv")
    elderly_pop = read_csv_safely(
        BASE_DIR / "202504_202510_ì£¼ë¯¼ë“±ë¡ì¸êµ¬ê¸°íƒ€í˜„í™©(ê³ ë ¹ ì¸êµ¬í˜„í™©)_ì›”ê°„.csv"
    )
    return {
        "air": air,
        "grade": grade,
        "region": region,
        "elderly": elderly,
        "chem": chem,
        "elderly_pop": elderly_pop,
        "cai": cai,
    }


def add_air_quality_grades(df_air: pd.DataFrame,
                           df_grade: pd.DataFrame) -> pd.DataFrame:
    """ëŒ€ê¸°ì§ˆ ì›”í‰ê·  ë°ì´í„°ì— í•­ëª©ë³„ ë“±ê¸‰/ì ìˆ˜/ì¢…í•©ìœ„í—˜ì ìˆ˜ ì¶”ê°€."""
    df = df_air.copy()

    # ì¸¡ì •ì¼ì(YYYYMM)ë¥¼ ì‹¤ì œ ë‚ ì§œ(ë§¤ì›” 1ì¼)ë¡œ ë³€í™˜
    df["ì¸¡ì •ì¼"] = pd.to_datetime(df["ì¸¡ì •ì¼ì"].astype(str), format="%Y%m")

    # í•­ëª©ë³„ ë“±ê¸‰ ê¸°ì¤€ (í•­ëª©ëª… ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°)
    grade_info = df_grade.drop_duplicates("í•­ëª©ëª…", keep="first").set_index("í•­ëª©ëª…")

    def calc_grade(value: float, standard_row: pd.Series) -> str:
        if pd.isna(value):
            return np.nan
        if value <= standard_row["ì¢‹ìŒê¸°ì¤€"]:
            return "ì¢‹ìŒ"
        elif value <= standard_row["ë³´í†µê¸°ì¤€"]:
            return "ë³´í†µ"
        elif value <= standard_row["ë‚˜ì¨ê¸°ì¤€"]:
            return "ë‚˜ì¨"
        else:
            return "ë§¤ìš°ë‚˜ì¨"

    # ì˜¤ì—¼ë¬¼ì§ˆë³„ ë“±ê¸‰/ì ìˆ˜ ê³„ì‚°
    for pollutant, col_name in POLLUTANT_COLS.items():
        thresholds = grade_info.loc[pollutant]
        grade_col = f"{pollutant}_ë“±ê¸‰"
        score_col = f"{pollutant}_ì ìˆ˜"

        df[grade_col] = df[col_name].apply(lambda v: calc_grade(v, thresholds))
        df[score_col] = df[grade_col].map(GRADE_TO_SCORE)

    # ì¢…í•©ìœ„í—˜ì ìˆ˜: 6ê°œ í•­ëª© ì ìˆ˜ ì¤‘ ìµœëŒ“ê°’(=ê°€ì¥ ë‚˜ìœ ë“±ê¸‰)
    score_cols = [c for c in df.columns if c.endswith("_ì ìˆ˜")]
    df["ì¢…í•©ìœ„í—˜ì ìˆ˜"] = df[score_cols].max(axis=1)

    return df


def make_city_summary(df_air_scored: pd.DataFrame) -> pd.DataFrame:
    """ë„ì‹œë³„ í‰ê·  ë†ë„ / í‰ê·  ì¢…í•©ìœ„í—˜ì ìˆ˜ ìš”ì•½."""
    agg_cols = {
        "ì´ì‚°í™”í™©ì¸¡ì •ê°’(ppm)": "mean",
        "ì´ì‚°í™”ì§ˆì†Œì¸¡ì •ê°’(ppm)": "mean",
        "ì¼ì‚°í™”íƒ„ì†Œì¸¡ì •ê°’(ppm)": "mean",
        "ì˜¤ì¡´ì¸¡ì •ê°’(ppm)": "mean",
        "PM10ì¸¡ì •ê°’(ã/ã¥)": "mean",
        "PM25ì¸¡ì •ê°’(ã/ã¥)": "mean",
        "ì¢…í•©ìœ„í—˜ì ìˆ˜": "mean",
    }
    city_summary = (
        df_air_scored
        .groupby("ë„ì‹œëª…")
        .agg(agg_cols)
        .rename_axis("ë„ì‹œëª…")
        .reset_index()
    )
    return city_summary

# í–‰ì • ìÂ·ë©´Â·ë™ ì¶”ì¶œ : í‰íƒì‹œ ë²•ì • 25ê°œë§Œ í—ˆìš©
# í–‰ì • ìÂ·ë©´Â·ë™ë§Œ ë½‘ëŠ” í•¨ìˆ˜ (ê±´ë¬¼ë™ í•„í„°ë§ + ë¹„ì „/ì‹ ì¥ í†µí•©)
def extract_eupmyeondong(addr: str) -> str:
    if pd.isna(addr):
        return np.nan

    text = str(addr)

    # 0ë‹¨ê³„: ì£¼ì†Œ ì „ì²´ì—ì„œ ë¹„ì „/ì‹ ì¥ ê³„ì—´ ë¨¼ì € ì²˜ë¦¬
    #   ì˜ˆ: "ë¹„ì „1ë™ 123-4", "ì‹ ì¥ 2ë™ 11-3" ë“±
    for key, canon in EMD_ALIAS_MAP.items():
        if key in text:
            return canon

    # 1ë‹¨ê³„: ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ í† í° ë‹¨ìœ„ í•„í„°ë§
    # ê³µë°±, ì‰¼í‘œ, ê´„í˜¸ ê¸°ì¤€ìœ¼ë¡œ í† í° ë¶„ë¦¬
    tokens = re.split(r"[ ,()]", text)

    for tok in tokens:
        tok = tok.strip()
        if not tok:
            continue

        # ì/ë©´/ë™ìœ¼ë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ íŒ¨ìŠ¤
        if not tok.endswith(("ì", "ë©´", "ë™")):
            continue

        # ê´‘ì—­/ê¸°ì´ˆ ì§€ìì²´ ì´ë¦„ ì œì™¸
        if tok in ("ê²½ê¸°ë„", "í‰íƒì‹œ"):
            continue

        # ìˆ«ì + ë™ (1ë™, 103ë™ ë“±) â†’ ê±´ë¬¼ë™
        if re.fullmatch(r"\d+ë™", tok):
            continue

        # ì˜ë¬¸/ìˆ«ì ì½”ë“œ + ë™ (Aë™, Bë™, S001ë™ ë“±) â†’ ê±´ë¬¼ë™
        if re.fullmatch(r"[A-Za-z0-9]+ë™", tok):
            continue

        # ì œ1ë™, ì œ2ë™ í˜•íƒœ â†’ ê±´ë¬¼ë™
        if re.fullmatch(r"ì œ\d+ë™", tok):
            continue

        # ìƒê°€ë™ ê´€ë ¨ â†’ ê±´ë¬¼ë™
        if "ìƒê°€ë™" in tok or (tok.startswith("ìƒê°€") and tok.endswith("ë™")):
            continue

        # í•œ ê¸€ì + ë™ (ê°€ë™, ë‚˜ë™ ë“±) â†’ ê±´ë¬¼ë™
        if len(tok) == 2 and tok.endswith("ë™"):
            continue

        # ë¹„ì „1ë™/ì‹ ì¥1ë™ ë“±ì´ í† í°ìœ¼ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°ë¥¼ ì •ê·œí™”
        norm = EMD_ALIAS_MAP.get(tok, tok)

        # 23ê°œ ë²•ì •ë™ ì•ˆì— ë“¤ì–´ê°€ëŠ” ê²ƒë§Œ ì¸ì •
        if norm in LEGAL_EMD:
            return norm

    return np.nan


# ------------------------------------------------------------
# ë…¸ì¸ë³µì§€ì‹œì„¤ ë„ë¡œëª…ì£¼ì†Œ ì§€ì˜¤ì½”ë”© (OpenStreetMap Nominatim ì˜ˆì‹œ)
# ------------------------------------------------------------
def _geocode_single(addr: str):
    """ë‹¨ì¼ ì£¼ì†Œë¥¼ ìœ„ë„/ê²½ë„ë¡œ ë³€í™˜ (ì‹¤íŒ¨í•˜ë©´ (None, None) ë°˜í™˜)."""
    if not addr or pd.isna(addr):
        return None, None

    url = "https://nominatim.openstreetmap.org/search"
    params = {
        "q": addr,
        "format": "json",
        "limit": 1,
    }
    headers = {
        # OSM ì •ì±…ìƒ user-agent ê¼­ í•„ìš”. ì´ë©”ì¼ì€ í¸í•œ ê±¸ë¡œ ë°”ê¿”ë„ ë¨.
        "User-Agent": "pyeongtaek-esg-app (contact: your_email@example.com)"
    }

    try:
        r = requests.get(url, params=params, headers=headers, timeout=5)
        r.raise_for_status()
        data = r.json()
        if not data:
            return None, None
        return float(data[0]["lat"]), float(data[0]["lon"])
    except Exception:
        return None, None


@st.cache_data
def geocode_elderly_addresses(addresses: tuple) -> pd.DataFrame:
    """
    ë…¸ì¸ë³µì§€ì‹œì„¤ ë„ë¡œëª…ì£¼ì†Œ ëª©ë¡ì„ ë°›ì•„ ì§€ì˜¤ì½”ë”© ê²°ê³¼(lat, lon)ë¥¼ ë°˜í™˜.
    ê°™ì€ ì£¼ì†Œ ì„¸íŠ¸ì— ëŒ€í•´ì„œëŠ” ìºì‹œë˜ì–´ ë‹¤ì‹œ í˜¸ì¶œë˜ì§€ ì•ŠìŒ.
    """
    rows = []
    for addr in addresses:
        lat, lon = _geocode_single(addr)
        rows.append({"ë„ë¡œëª…ì£¼ì†Œ": addr, "ìœ„ë„": lat, "ê²½ë„": lon})
        # ë„ˆë¬´ ê³µê²©ì ìœ¼ë¡œ í˜¸ì¶œí•˜ë©´ ë§‰í ìˆ˜ ìˆìœ¼ë‹ˆ ì•½ê°„ì˜ ê°„ê²©
        time.sleep(0.3)
    return pd.DataFrame(rows)


def ensure_elderly_geocoded(df_elderly: pd.DataFrame) -> pd.DataFrame:
    """
    ë…¸ì¸ë³µì§€ì‹œì„¤ ë°ì´í„°ì— ìœ„ë„/ê²½ë„ ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜ ì „ë¶€ ê²°ì¸¡ì´ë©´
    ë„ë¡œëª…ì£¼ì†Œ ê¸°ì¤€ìœ¼ë¡œ ì§€ì˜¤ì½”ë”©ì„ ìˆ˜í–‰í•´ lat/lonì„ ë¶™ì—¬ì¤Œ.
    """
    df = df_elderly.copy()

    # ì´ë¯¸ ìœ„ë„/ê²½ë„ ìˆê³  ì–´ëŠ ì •ë„ ì±„ì›Œì ¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if {"ìœ„ë„", "ê²½ë„"}.issubset(df.columns) and not df[["ìœ„ë„", "ê²½ë„"]].isna().all().all():
        return df

    # ë„ë¡œëª…ì£¼ì†Œì—ì„œ ìœ ë‹ˆí¬ ì£¼ì†Œë§Œ ì¶”ì¶œ
    addr_series = df["ë„ë¡œëª…ì£¼ì†Œ"].dropna().unique().tolist()
    if len(addr_series) == 0:
        return df

    addr_tuple = tuple(sorted(addr_series))

    with st.spinner("ë…¸ì¸ë³µì§€ì‹œì„¤ ë„ë¡œëª…ì£¼ì†Œë¥¼ ì§€ì˜¤ì½”ë”©í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. (í•œ ë²ˆë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
        geo_df = geocode_elderly_addresses(addr_tuple)

    df = df.merge(geo_df, on="ë„ë¡œëª…ì£¼ì†Œ", how="left")
    return df


# ------------------------------------------------------------
# ë©”ì¸ ëŒ€ì‹œë³´ë“œ
# ------------------------------------------------------------
def main():
    st.title("CSC í”„ë¡œì íŠ¸ - ê³µê³µ ESG ê´€ì  í‰íƒì‹œ ëŒ€ê¸°ì§ˆ ë¦¬ìŠ¤í¬ & ë…¸ì¸ë³µì§€ì‹œì„¤ ë¶„ì„")
    st.caption(
        "ë°ì´í„° ì¶œì²˜: ê³µê³µë°ì´í„°í¬í„¸(data.go.kr) - "
        "ê²½ê¸°ë„ ëŒ€ê¸°í™˜ê²½ì •ë³´, í‰íƒì‹œ ë…¸ì¸ë³µì§€ì‹œì„¤, ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ì‚¬ì—…ì¥, "
        "ê²½ê¸°ë„ ëŒ€ê¸°í™˜ê²½ ì§„ë‹¨í‰ê°€ì‹œìŠ¤í…œ ì§€ì—­ì •ë³´, ì£¼ë¯¼ë“±ë¡ì¸êµ¬(ê³ ë ¹ ì¸êµ¬í˜„í™©)"
    )

    # ë°ì´í„° ë¡œë“œ
    data = load_data()
    df_air_raw = data["air"]
    df_grade = data["grade"]
    df_region = data["region"]
    df_elderly_raw = data["elderly"]
    df_chem = data["chem"]
    df_pop = data["elderly_pop"]
    df_cai = data["cai"]

    # ë…¸ì¸ë³µì§€ì‹œì„¤: ë„ë¡œëª…ì£¼ì†Œ ì§€ì˜¤ì½”ë”© ì ìš©
    df_elderly = ensure_elderly_geocoded(df_elderly_raw)

    # ì „ì²˜ë¦¬ (ëŒ€ê¸°ì§ˆ ë“±ê¸‰/ìœ„í—˜ì ìˆ˜ ê³„ì‚°)
    df_air = add_air_quality_grades(df_air_raw, df_grade)
    city_summary = make_city_summary(df_air)

    # ğŸ‘‰ í‰íƒì‹œ ì¢…í•©ìœ„í—˜ì ìˆ˜ëŠ” CAI íŒŒì¼ ê°’ìœ¼ë¡œ ëŒ€ì²´
    # pyeongtaek_CAI_index.csv : ìÂ·ë©´Â·ë™ë³„ CAI_Index (ì´ë¯¸ ì¢…í•© ìœ„í—˜ì§€ìˆ˜ë¡œ ê³„ì‚°ëœ ê°’)
    if "CAI_Index" in df_cai.columns:
        # í‰íƒì‹œ 23ê°œ ìÂ·ë©´Â·ë™ CAI_Indexì˜ í‰ê· ì„ 'í‰íƒì‹œ ì¢…í•©ìœ„í—˜ì ìˆ˜'ë¡œ ì‚¬ìš©
        pyeongtaek_cai_mean = df_cai["CAI_Index"].mean()
        city_summary.loc[
            city_summary["ë„ì‹œëª…"] == "í‰íƒì‹œ", "ì¢…í•©ìœ„í—˜ì ìˆ˜"
        ] = pyeongtaek_cai_mean

    # í‰íƒì‹œ/ê²½ê¸°ë„ í‰ê·  ìœ„í—˜ ì ìˆ˜ (ìœ„ ì½”ë“œì—ì„œ í‰íƒì‹œ ê°’ ì´ë¯¸ êµì²´ë¨)
    pyeongtaek_row = city_summary[city_summary["ë„ì‹œëª…"] == "í‰íƒì‹œ"].iloc[0]
    gyeonggi_mean_risk = city_summary["ì¢…í•©ìœ„í—˜ì ìˆ˜"].mean()
    pyeongtaek_risk = pyeongtaek_row["ì¢…í•©ìœ„í—˜ì ìˆ˜"]

    # í‰íƒì‹œ ê¸°ì´ˆ ì •ë³´ (ëŒ€ê¸°í™˜ê²½ ì§„ë‹¨í‰ê°€ì‹œìŠ¤í…œ)
    region_row = df_region[df_region["ì‹œêµ°êµ¬ëª…"] == "í‰íƒì‹œ"].iloc[0]

    # í‰íƒì‹œ ë‚´ë¶€ ìÂ·ë©´Â·ë™ ë‹¨ìœ„ 'ìœ„í—˜ì§€ìˆ˜' ê³„ì‚°
    # ë…¸ì¸ë³µì§€ì‹œì„¤: ë„ë¡œëª…ì£¼ì†Œ ì‚¬ìš©
    # ë…¸ì¸ë³µì§€ì‹œì„¤: ë„ë¡œëª…ì£¼ì†Œ ì‚¬ìš©
    df_elderly["í–‰ì •ë™"] = df_elderly["ë„ë¡œëª…ì£¼ì†Œ"].apply(extract_eupmyeondong)

    # ìœ í•´í™”í•™ë¬¼ì§ˆ ì‚¬ì—…ì¥: ë„ë¡œëª…ì£¼ì†Œ â†’ ì•ˆ ë‚˜ì˜¤ë©´ ì§€ë²ˆì£¼ì†Œë¡œ ë³´ì™„
    df_chem["í–‰ì •ë™"] = df_chem["ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ"].apply(extract_eupmyeondong)
    mask_na = df_chem["í–‰ì •ë™"].isna()
    if "ì†Œì¬ì§€ì§€ë²ˆì£¼ì†Œ" in df_chem.columns:
        df_chem.loc[mask_na, "í–‰ì •ë™"] = df_chem.loc[mask_na, "ì†Œì¬ì§€ì§€ë²ˆì£¼ì†Œ"].apply(
            extract_eupmyeondong
        )

    # ğŸ‘‰ í‰íƒì‹œ ë²•ì •ë™ 23ê°œ ê¸°ì¤€ìœ¼ë¡œ ê°•ì œ ì •ë ¬/ì±„ì›€
    emd_index = pd.Index(LEGAL_EMD, name="í–‰ì •ë™")

    elderly_cnt = (
        df_elderly.groupby("í–‰ì •ë™")
        .size()
        .rename("ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜")
    )
    chem_cnt = (
        df_chem.groupby("í–‰ì •ë™")
        .size()
        .rename("ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜")
    )

    # 0ë‹¨ê³„: ê¸°ë³¸ ì§‘ê³„ (ì‹œì„¤ ìˆ˜)
    local_risk = (
        pd.concat([elderly_cnt, chem_cnt], axis=1)
        .reindex(emd_index)   # 23ê°œ ë™ ëª¨ë‘ í¬í•¨
        .fillna(0)
    )

    local_risk["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"] = local_risk["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"].astype(int)
    local_risk["ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜"] = local_risk["ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜"].astype(int)

    # 1ë‹¨ê³„: CAI íŒŒì¼ì—ì„œ ìÂ·ë©´Â·ë™ë³„ 'ìµœì¢… ìœ„í—˜ì§€ìˆ˜' ê°€ì ¸ì˜¤ê¸°
    #  - pyeongtaek_CAI_index.csv : [ìë©´ë™, CAI_Index, CAI_ë“±ê¸‰]
    cai_index = df_cai.set_index("ìë©´ë™")

    # í–‰ì •ë™ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ CAI_Indexë¥¼ ë¶™ì„
    local_risk = local_risk.join(cai_index[["CAI_Index"]], how="left")

    # 2ë‹¨ê³„: ì¢…í•© ìœ„í—˜ì§€ìˆ˜ = CAI_Index (íŒŒì¼ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    local_risk["ìœ„í—˜ì§€ìˆ˜"] = local_risk["CAI_Index"]

    # CAI_Index ì¤‘ê°„ ì»¬ëŸ¼ì€ ì•ˆ ë³´ì—¬ë„ ë˜ë‹ˆ ì œê±°
    local_risk = local_risk.drop(columns=["CAI_Index"])

    # ìµœì¢… ìœ„í—˜ì§€ìˆ˜ ê¸°ì¤€ ì •ë ¬
    local_risk = local_risk.sort_values("ìœ„í—˜ì§€ìˆ˜", ascending=False)

    # ìÂ·ë©´Â·ë™ë³„ í‰ê·  ì¢Œí‘œ (ë…¸ì¸ë³µì§€ì‹œì„¤ + ìœ í•´í™”í•™ì‚¬ì—…ì¥ ëª¨ë‘ í™œìš©)
    coords_list = []
    if {"ìœ„ë„", "ê²½ë„"}.issubset(df_elderly.columns):
        elder_coords_all = (
            df_elderly.dropna(subset=["ìœ„ë„", "ê²½ë„"])
            .groupby("í–‰ì •ë™")[["ìœ„ë„", "ê²½ë„"]]
            .mean()
        )
        coords_list.append(elder_coords_all)

    if {"ìœ„ë„", "ê²½ë„"}.issubset(df_chem.columns):
        chem_coords = (
            df_chem.dropna(subset=["ìœ„ë„", "ê²½ë„"])
            .groupby("í–‰ì •ë™")[["ìœ„ë„", "ê²½ë„"]]
            .mean()
        )
        coords_list.append(chem_coords)

    if coords_list:
        coords_all = (
            pd.concat(coords_list)
            .groupby("í–‰ì •ë™")[["ìœ„ë„", "ê²½ë„"]]
            .mean()
        )
        local_risk_map = local_risk.join(coords_all, how="left")
    else:
        coords_all = pd.DataFrame()
        local_risk_map = local_risk.copy()

    # ì£¼ë¯¼ë“±ë¡ ì¸êµ¬(ê³ ë ¹ ì¸êµ¬) - í‰íƒì‹œ ìÂ·ë©´Â·ë™ë³„ 65ì„¸ ì´ìƒ ì¸êµ¬
    df_pop_pt = df_pop[df_pop["í–‰ì •êµ¬ì—­"].str.contains("í‰íƒì‹œ", na=False)].copy()
    df_pop_pt["ìë©´ë™"] = df_pop_pt["í–‰ì •êµ¬ì—­"].apply(extract_eupmyeondong)
    df_pop_pt = df_pop_pt[~df_pop_pt["ìë©´ë™"].isna()].copy()

    aged_total_cols = [c for c in df_pop_pt.columns if "65ì„¸ì´ìƒì „ì²´" in c]
    aged_total_cols = sorted(aged_total_cols)
    default_month_idx = len(aged_total_cols) - 1 if aged_total_cols else 0

    # --------------------------------------------------------
    # íƒ­ êµ¬ì„±
    # --------------------------------------------------------
    tabs = st.tabs([
        "1. ë°ì´í„° ê°œìš”",
        "2. ëŒ€ê¸°ì§ˆ ë¶„ì„ (ê²½ê¸°ë„ vs í‰íƒì‹œ)",
        "3. í‰íƒì‹œ ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ ì‚¬ì—…ì¥",
        "4. í‰íƒì‹œ ë…¸ì¸ë³µì§€ì‹œì„¤ ë¶„í¬",
        "5. ìœ„í—˜ì§€ìˆ˜ ë¶„ì„",
        "6. ê³µê³µ ESG ê´€ì  ì¢…í•© ì§„ë‹¨",
    ])
    # --------------------------------------------------------
    # 1. ë°ì´í„° ê°œìš”
    # --------------------------------------------------------
    with tabs[0]:
        st.subheader("ë°ì´í„° ê°œìš”")
        c1, c2, c3 = st.columns(3)
        c1.metric("ëŒ€ê¸°ì§ˆ ì›”í‰ê·  ë°ì´í„° (í–‰)", f"{len(df_air):,}")
        c2.metric("ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜", f"{len(df_elderly):,}")
        c3.metric("ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ ì‚¬ì—…ì¥ ìˆ˜", f"{len(df_chem):,}")

        st.markdown("#### (1) ëŒ€ê¸°ì§ˆ ì›”í‰ê·  ë°ì´í„° ì˜ˆì‹œ")
        st.dataframe(
            df_air[
                [
                    "ë„ì‹œëª…", "ì¸¡ì •ì¥ì†Œëª…", "ì¸¡ì •ì¼",
                    "PM10ì¸¡ì •ê°’(ã/ã¥)", "PM25ì¸¡ì •ê°’(ã/ã¥)",
                    "ì˜¤ì¡´ì¸¡ì •ê°’(ppm)", "ì¢…í•©ìœ„í—˜ì ìˆ˜"
                ]
            ].head(20),
            use_container_width=True,
        )

        st.markdown("#### (2) ë…¸ì¸ë³µì§€ì‹œì„¤ ë°ì´í„° ì˜ˆì‹œ")
        st.dataframe(df_elderly.head(20), use_container_width=True)

        st.markdown("#### (3) ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ ì‚¬ì—…ì¥ ë°ì´í„° ì˜ˆì‹œ")
        st.dataframe(df_chem.head(20), use_container_width=True)

        st.markdown("#### (4) ì£¼ë¯¼ë“±ë¡ ê³ ë ¹ ì¸êµ¬ ë°ì´í„° ì˜ˆì‹œ (í‰íƒì‹œ)")
        st.dataframe(df_pop_pt.head(20), use_container_width=True)

        st.caption("â€» ì¢…í•©ìœ„í—˜ì ìˆ˜: ê° ì›”/ì¸¡ì •ì†Œë³„ 6ê°œ ì˜¤ì—¼ë¬¼ì§ˆ ì ìˆ˜(1~4) ì¤‘ ìµœëŒ“ê°’")

    # --------------------------------------------------------
    # 2. ëŒ€ê¸°ì§ˆ ë¶„ì„
    # --------------------------------------------------------
    with tabs[1]:
        st.subheader("ê²½ê¸°ë„ / í‰íƒì‹œ ëŒ€ê¸°ì§ˆ ë¹„êµ ë° ì¶”ì´ ë¶„ì„")

        city_list = sorted(df_air["ë„ì‹œëª…"].unique())
        default_city_idx = city_list.index("í‰íƒì‹œ") if "í‰íƒì‹œ" in city_list else 0

        left, right = st.columns([2, 3])

        with left:
            sel_city = st.selectbox("ë„ì‹œ ì„ íƒ", city_list, index=default_city_idx)

            df_city = df_air[df_air["ë„ì‹œëª…"] == sel_city].copy()
            site_list = sorted(df_city["ì¸¡ì •ì¥ì†Œëª…"].unique())
            sel_site = st.selectbox("ì¸¡ì •ì†Œ ì„ íƒ", site_list)

            pollutant_options = list(POLLUTANT_COLS.keys())
            sel_pollutant = st.selectbox(
                "ì˜¤ì—¼ë¬¼ì§ˆ ì„ íƒ",
                pollutant_options,
                format_func=lambda x: POLLUTANT_LABELS.get(x, x),
            )

            df_site = (
                df_city[df_city["ì¸¡ì •ì¥ì†Œëª…"] == sel_site]
                .sort_values("ì¸¡ì •ì¼")
            )

            value_col = POLLUTANT_COLS[sel_pollutant]

        with right:
            st.markdown(
                f"##### [{sel_city} - {sel_site}] {POLLUTANT_LABELS.get(sel_pollutant, sel_pollutant)} ì›”ë³„ ì¶”ì´"
            )

            plot_df = df_site.set_index("ì¸¡ì •ì¼")[[value_col]]
            plot_df.columns = ["ë†ë„"]
            st.line_chart(plot_df)

        st.markdown("----")
        st.markdown("#### ë„ì‹œë³„ í‰ê·  ë†ë„ ë° ì¢…í•©ìœ„í—˜ì ìˆ˜ (ê²½ê¸°ë„ ì „ì²´)")

        st.dataframe(
            city_summary.sort_values("ì¢…í•©ìœ„í—˜ì ìˆ˜", ascending=False),
            use_container_width=True,
        )

    # --------------------------------------------------------
    # 3. í‰íƒì‹œ ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ ì‚¬ì—…ì¥
    # --------------------------------------------------------
    with tabs[2]:
        st.subheader("í‰íƒì‹œ ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ ì‚¬ì—…ì¥ í˜„í™©")

        st.metric("ì‚¬ì—…ì¥ ìˆ˜", f"{len(df_chem):,}")

        industry_all = sorted(df_chem["ì—…ì¢…ëª…"].unique())
        selected_industries = st.multiselect(
            "ì—…ì¢… í•„í„° (ì„ íƒ ì•ˆ í•˜ë©´ ì „ì²´)",
            industry_all,
        )
        if selected_industries:
            df_chem_view = df_chem[df_chem["ì—…ì¢…ëª…"].isin(selected_industries)].copy()
        else:
            df_chem_view = df_chem.copy()

        st.markdown("#### (1) ì‚¬ì—…ì¥ ìœ„ì¹˜ (ìœ„ë„/ê²½ë„ ê¸°ë°˜)")
        if {"ìœ„ë„", "ê²½ë„"}.issubset(df_chem_view.columns):
            map_df = df_chem_view.rename(columns={"ìœ„ë„": "lat", "ê²½ë„": "lon"})
            st.map(map_df[["lat", "lon"]])
        else:
            st.info("ìœ„ë„/ê²½ë„ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ ì‹œê°í™”ëŠ” ìƒëµí•©ë‹ˆë‹¤.")

        st.markdown("#### (2) ìƒì„¸ í…Œì´ë¸”")
        st.dataframe(df_chem_view.reset_index(drop=True), use_container_width=True)
    # --------------------------------------------------------
    # 4. í‰íƒì‹œ ë…¸ì¸ë³µì§€ì‹œì„¤ ë¶„í¬ (ì§€ë„ ì‹œê°í™” + ì¶©ì¡±ë„ ë°ì´í„°)
    # --------------------------------------------------------
    with tabs[3]:
        st.subheader("í‰íƒì‹œ ë…¸ì¸ë³µì§€ì‹œì„¤ í˜„í™©")

        st.metric("ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜", f"{len(df_elderly):,}")

        # (1) ë…¸ì¸ë³µì§€ì‹œì„¤ ìœ„ì¹˜ ì§€ë„ ----------------------------------------
        st.markdown("#### (1) ë…¸ì¸ë³µì§€ì‹œì„¤ ìœ„ì¹˜ ì§€ë„")
        if {"ìœ„ë„", "ê²½ë„"}.issubset(df_elderly.columns) and not df_elderly[["ìœ„ë„", "ê²½ë„"]].isna().all().all():
            elder_map = (
                df_elderly
                .dropna(subset=["ìœ„ë„", "ê²½ë„"])
                .rename(columns={"ìœ„ë„": "lat", "ê²½ë„": "lon"})
            )
            st.map(elder_map[["lat", "lon"]])
        else:
            st.info(
                "ë…¸ì¸ë³µì§€ì‹œì„¤ ë„ë¡œëª…ì£¼ì†Œ ì§€ì˜¤ì½”ë”© ê²°ê³¼ê°€ ì—†ì–´ì„œ ì§€ë„ë¥¼ í‘œì‹œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            )

        # (2) ìÂ·ë©´Â·ë™ë³„ 65ì„¸ ì´ìƒ ì¸êµ¬ ëŒ€ë¹„ ë…¸ì¸ë³µì§€ì‹œì„¤ ì¶©ì¡±ë„ -------------
        st.markdown("#### (2) ìÂ·ë©´Â·ë™ë³„ 65ì„¸ ì´ìƒ ì¸êµ¬ ëŒ€ë¹„ ë…¸ì¸ë³µì§€ì‹œì„¤ ì¶©ì¡±ë„")

        if aged_total_cols:
            # ê¸°ì¤€ ì›” ì„ íƒ
            month_label_map = {col: col.replace("_65ì„¸ì´ìƒì „ì²´", "") for col in aged_total_cols}
            month_labels = list(month_label_map.values())
            sel_label = st.selectbox(
                "ê¸°ì¤€ ì›” ì„ íƒ (65ì„¸ ì´ìƒ ì¸êµ¬)",
                month_labels,
                index=default_month_idx,
            )
            inv_month_label_map = {v: k for k, v in month_label_map.items()}
            sel_col = inv_month_label_map[sel_label]

            # í‰íƒì‹œ ìÂ·ë©´Â·ë™ë³„ 65ì„¸ ì´ìƒ ì¸êµ¬
            pop_month = (
                df_pop_pt[["ìë©´ë™", sel_col]]
                .assign(
                    ê³ ë ¹ì¸êµ¬_ìˆ˜=lambda d: d[sel_col]
                    .replace(",", "", regex=True)
                    .astype("int64")
                )[["ìë©´ë™", "ê³ ë ¹ì¸êµ¬_ìˆ˜"]]
                .groupby("ìë©´ë™")["ê³ ë ¹ì¸êµ¬_ìˆ˜"]
                .sum()
                .rename_axis("í–‰ì •ë™")
            )

            # ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜ (í–‰ì •ë™ ê¸°ì¤€)
            elderly_cnt_for_cov = (
                df_elderly.groupby("í–‰ì •ë™")
                .size()
                .rename("ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜")
            )

            coverage = pd.concat([elderly_cnt_for_cov, pop_month], axis=1)
            coverage["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"] = coverage["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"].fillna(0).astype(int)
            coverage["ê³ ë ¹ì¸êµ¬_ìˆ˜"] = coverage["ê³ ë ¹ì¸êµ¬_ìˆ˜"].fillna(0).astype(int)

            # 65ì„¸ ì´ìƒ 1ì²œ ëª…ë‹¹ ì‹œì„¤ ìˆ˜
            coverage["ì‹œì„¤_ì²œëª…ë‹¹"] = np.where(
                coverage["ê³ ë ¹ì¸êµ¬_ìˆ˜"] > 0,
                coverage["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"] / (coverage["ê³ ë ¹ì¸êµ¬_ìˆ˜"] / 1000.0),
                np.nan,
            )

            # ì§€ë„ìš© ì¢Œí‘œ (í–‰ì •ë™ë³„ í‰ê·  ìœ„ë„/ê²½ë„)
            if {"ìœ„ë„", "ê²½ë„"}.issubset(df_elderly.columns):
                coords_cov = (
                    df_elderly
                    .dropna(subset=["ìœ„ë„", "ê²½ë„"])
                    .groupby("í–‰ì •ë™")[["ìœ„ë„", "ê²½ë„"]]
                    .mean()
                    .rename(columns={"ìœ„ë„": "lat", "ê²½ë„": "lon"})
                )
                coverage_with_coords = (
                    coverage.join(coords_cov, how="left")
                    .reset_index()
                    .rename(columns={"í–‰ì •ë™": "ìë©´ë™"})
                )
            else:
                coverage_with_coords = (
                    coverage.reset_index()
                    .rename(columns={"í–‰ì •ë™": "ìë©´ë™"})
                )

            # ì§€ë„ í‘œì‹œ (ì¢Œí‘œì™€ ì¶©ì¡±ë„ ëª¨ë‘ ìˆëŠ” í–‰ë§Œ ì‚¬ìš©)
            if {"lat", "lon"}.issubset(coverage_with_coords.columns):
                cov_for_map = coverage_with_coords.dropna(subset=["lat", "lon", "ì‹œì„¤_ì²œëª…ë‹¹"])
                if not cov_for_map.empty:
                    max_cov = float(cov_for_map["ì‹œì„¤_ì²œëª…ë‹¹"].max())
                    min_radius, max_radius = 300, 1400
                    cov_for_map["marker_radius"] = (
                        min_radius
                        + (cov_for_map["ì‹œì„¤_ì²œëª…ë‹¹"] / max_cov) * (max_radius - min_radius)
                    )

                    layer = pdk.Layer(
                        "ScatterplotLayer",
                        data=cov_for_map,
                        get_position="[lon, lat]",
                        get_radius="marker_radius",
                        get_fill_color="[0, 153, 255, 150]",
                        pickable=True,
                    )
                    view_state = pdk.ViewState(
                        latitude=float(cov_for_map["lat"].mean()),
                        longitude=float(cov_for_map["lon"].mean()),
                        zoom=10.5,
                        pitch=0,
                    )
                    st.pydeck_chart(
                        pdk.Deck(
                            layers=[layer],
                            initial_view_state=view_state,
                            tooltip={
                                "text": "ìÂ·ë©´Â·ë™: {ìë©´ë™}\\n"
                                        "65ì„¸ ì´ìƒ ì¸êµ¬: {ê³ ë ¹ì¸êµ¬_ìˆ˜}ëª…\\n"
                                        "ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜: {ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜}ê°œ\\n"
                                        "ì‹œì„¤ ìˆ˜ (ì²œ ëª…ë‹¹): {ì‹œì„¤_ì²œëª…ë‹¹:.2f}"
                            },
                        )
                    )
                    st.caption(
                        f"â€» ê¸°ì¤€ ì›”: **{sel_label}**, 65ì„¸ ì´ìƒ 1ì²œ ëª…ë‹¹ ì‹œì„¤ ìˆ˜ê°€ í´ìˆ˜ë¡ "
                        "ë…¸ì¸ë³µì§€ ì¸í”„ë¼ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì˜ ê°–ì¶°ì§„ ì§€ì—­ì…ë‹ˆë‹¤."
                    )

            # âœ… ì§€ì˜¤ì½”ë”© + ì¶©ì¡±ë„ ê²°ê³¼ ë°ì´í„° í…Œì´ë¸” í•­ìƒ ë³´ì—¬ì£¼ê¸°
            st.markdown("#### (3) ìÂ·ë©´Â·ë™ë³„ ë…¸ì¸ë³µì§€ì‹œì„¤ ì¶©ì¡±ë„ ë°ì´í„°")
            cols_to_show = ["ìë©´ë™", "ê³ ë ¹ì¸êµ¬_ìˆ˜", "ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜", "ì‹œì„¤_ì²œëª…ë‹¹"]
            # ì¢Œí‘œê¹Œì§€ ê°™ì´ ë³´ê³  ì‹¶ìœ¼ë©´ lat/lonë„ í¬í•¨
            if "lat" in coverage_with_coords.columns and "lon" in coverage_with_coords.columns:
                cols_to_show += ["lat", "lon"]

            st.dataframe(
                coverage_with_coords[cols_to_show].sort_values(
                    "ì‹œì„¤_ì²œëª…ë‹¹", ascending=False
                ),
                use_container_width=True,
            )
        else:
            st.info("ì£¼ë¯¼ë“±ë¡ ì¸êµ¬ í†µê³„ ë°ì´í„°ì—ì„œ '65ì„¸ì´ìƒì „ì²´' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # (4) ë„ë¡œëª…ì£¼ì†Œ ê²€ìƒ‰ --------------------------------------------
        st.markdown("#### (4) ë„ë¡œëª…ì£¼ì†Œ ê²€ìƒ‰")
        addr_query = st.text_input("ë„ë¡œëª…ì£¼ì†Œì— í¬í•¨ë  í‚¤ì›Œë“œ (ì˜ˆ: ê³ ë•, ì•ˆì¤‘, ì²­ë¶ ë“±)")
        df_elderly_view = df_elderly.copy()
        if addr_query:
            df_elderly_view = df_elderly_view[
                df_elderly_view["ë„ë¡œëª…ì£¼ì†Œ"].str.contains(addr_query, na=False)
            ]

        st.dataframe(df_elderly_view.reset_index(drop=True), use_container_width=True)

    # --------------------------------------------------------
    # 5. ìœ„í—˜ì§€ìˆ˜ ë¶„ì„ (ìÂ·ë©´Â·ë™ ë‹¨ìœ„)
    # --------------------------------------------------------
    with tabs[4]:
        st.subheader("í‰íƒì‹œ ìÂ·ë©´Â·ë™ë³„ í™˜ê²½ ìœ„í—˜ì§€ìˆ˜ ë¶„ì„")

        st.markdown("#### (1) í‰íƒì‹œ ëŒ€ê¸°í™˜ê²½ ì§„ë‹¨í‰ê°€ì‹œìŠ¤í…œ ì •ë³´")
        st.write(f"- ì§€ì—­ êµ¬ë¶„: **{region_row['ì§€ì—­']}**")
        st.write(f"- ì‹œêµ°êµ¬ëª…: **{region_row['ì‹œêµ°êµ¬ëª…']}**")
        st.write(f"- ì§€í˜• ì½”ë“œ: **{region_row['ì§€í˜•']}**")

        st.markdown("#### (2) ë„ì‹œë³„ ì¢…í•©ìœ„í—˜ì ìˆ˜ ë¹„êµ (ìƒìœ„ â†’ í•˜ìœ„)")
        st.dataframe(
            city_summary.sort_values("ì¢…í•©ìœ„í—˜ì ìˆ˜", ascending=False),
            use_container_width=True,
        )

        st.markdown("#### (3) í‰íƒì‹œ ìÂ·ë©´Â·ë™ë³„ ìœ„í—˜ì§€ìˆ˜ ì§€ë„")

        if {"ìœ„ë„", "ê²½ë„"}.issubset(local_risk_map.columns) and not local_risk_map["ìœ„ë„"].isna().all():
            risk_map_df = local_risk_map.dropna(subset=["ìœ„ë„", "ê²½ë„"]).reset_index()
            risk_map_df = risk_map_df.rename(
                columns={"í–‰ì •ë™": "ìë©´ë™", "ìœ„ë„": "lat", "ê²½ë„": "lon"}
            )

            # ì› í¬ê¸°: ìœ„í—˜ì§€ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ 300~1300m
            max_risk = float(risk_map_df["ìœ„í—˜ì§€ìˆ˜"].max())
            min_radius = 300
            max_radius = 1300

            risk_map_df["marker_radius"] = (
                min_radius
                + (risk_map_df["ìœ„í—˜ì§€ìˆ˜"] / max_risk) * (max_radius - min_radius)
            )

            layer = pdk.Layer(
                "ScatterplotLayer",
                data=risk_map_df,
                get_position="[lon, lat]",
                get_radius="marker_radius",
                get_fill_color="[255, 0, 0, 140]",  # ì•½ê°„ íˆ¬ëª…í•œ ë¹¨ê°„ìƒ‰
                pickable=True,
            )

            view_state = pdk.ViewState(
                latitude=float(risk_map_df["lat"].mean()),
                longitude=float(risk_map_df["lon"].mean()),
                zoom=10.5,
                pitch=0,
            )

            st.pydeck_chart(
                pdk.Deck(
                    layers=[layer],
                    initial_view_state=view_state,
                    tooltip={"text": "ìÂ·ë©´Â·ë™: {ìë©´ë™}\nìœ„í—˜ì§€ìˆ˜: {ìœ„í—˜ì§€ìˆ˜}"},
                )
            )

            st.caption(
                "â€» ìœ„í—˜ì§€ìˆ˜ ì§€ë„ëŠ” ë…¸ì¸ë³µì§€ì‹œì„¤Â·ìœ í•´í™”í•™ì‚¬ì—…ì¥ ì£¼ì†Œë¥¼ ì§€ì˜¤ì½”ë”©í•˜ì—¬ ì–»ì€ ì¢Œí‘œ(ìœ„ë„Â·ê²½ë„)ë¥¼ "
                "ìÂ·ë©´Â·ë™ë³„ë¡œ í‰ê· ë‚¸ ìœ„ì¹˜ì— í‘œì‹œí•œ ê²ƒì…ë‹ˆë‹¤."
            )

        else:
            st.info(
                "ìœ„í—˜ì§€ìˆ˜ ì§€ë„ë¥¼ í‘œì‹œí•˜ë ¤ë©´ ìÂ·ë©´Â·ë™ë³„ ìœ„ë„/ê²½ë„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "ë…¸ì¸ë³µì§€ì‹œì„¤ ë° ìœ í•´í™”í•™ë¬¼ì§ˆ ì‚¬ì—…ì¥ ë„ë¡œëª…ì£¼ì†Œë¥¼ ì§€ì˜¤ì½”ë”©í•´ 'ìœ„ë„', 'ê²½ë„' ì—´ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”."
            )

        st.markdown("#### (4) í‰íƒì‹œ ìÂ·ë©´Â·ë™ë³„ ë…¸ì¸ë³µì§€ì‹œì„¤ Â· ìœ í•´í™”í•™ì‚¬ì—…ì¥ Â· ìœ„í—˜ì§€ìˆ˜")
        st.caption(
            "ìœ„í—˜ì§€ìˆ˜ = pyeongtaek_CAI_index.csvì—ì„œ ë¶ˆëŸ¬ì˜¨ ìÂ·ë©´Â·ë™ë³„ ì¢…í•© ìœ„í—˜ì§€ìˆ˜(CAI_Index) ê°’ "
            "(ì´ë¯¸ ìœ í•´í™”í•™ì‚¬ì—…ì¥Â·ëŒ€ê¸°ì§ˆ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ì‚¬ì „ì— ê³„ì‚°ëœ ì§€ìˆ˜)"
        )

        # ìœ„í—˜ì§€ìˆ˜ í…Œì´ë¸”
        st.dataframe(
            local_risk.reset_index().rename(columns={"í–‰ì •ë™": "ìÂ·ë©´Â·ë™"}),
            use_container_width=True,
        )

        # ìƒìœ„/í•˜ìœ„ ì§€ì—­ ìë™ ìš”ì•½
        top_risky = local_risk.sort_values("ìœ„í—˜ì§€ìˆ˜", ascending=False).head(3).index.tolist()
        top_safe = local_risk.sort_values("ìœ„í—˜ì§€ìˆ˜", ascending=True).head(3).index.tolist()

        st.markdown("#### (5) ì‹œê°ìë£Œ ê¸°ë°˜ ê²°ë¡  ìš”ì•½")
        st.markdown(
            f"""
            - **ì·¨ì•½ ì§€ì—­(ìœ„í—˜ì§€ìˆ˜ ìƒìœ„ 3)**: {", ".join(top_risky)}  
              â†’ ìœ í•´í™”í•™ë¬¼ì§ˆ ì·¨ê¸‰ì‚¬ì—…ì¥ ë°€ì§‘ë„ì™€ ëŒ€ê¸°ì§ˆ(CAI)ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚˜ìœ ì§€ì—­ìœ¼ë¡œ,  
                ë™ì‹œì— ë…¸ì¸ë³µì§€ì‹œì„¤ì´ ë¶€ì¡±í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ **ìš°ì„  ê´€ë¦¬ ëŒ€ìƒ ê¶Œì—­**ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

            - **ìƒëŒ€ì ìœ¼ë¡œ ì–‘í˜¸í•œ ì§€ì—­(ìœ„í—˜ì§€ìˆ˜ í•˜ìœ„ 3)**: {", ".join(top_safe)}  
              â†’ ë…¸ì¸ë³µì§€ì‹œì„¤ì´ ìƒëŒ€ì ìœ¼ë¡œ ì¶©ë¶„í•˜ê±°ë‚˜ ìœ í•´í™”í•™ì‚¬ì—…ì¥ ë°€ì§‘ë„ê°€ ë‚®ì€ ì§€ì—­ìœ¼ë¡œ,  
                ì‹ ê·œ ê³µê¸‰ë³´ë‹¤ëŠ” **ê¸°ì¡´ ì‹œì„¤ì˜ ì§ˆì  ê°œì„ ê³¼ ì„œë¹„ìŠ¤ ê³ ë„í™”** ì¤‘ì‹¬ì˜ ì „ëµì´ ì í•©í•©ë‹ˆë‹¤.  
            """
        )
    # --------------------------------------------------------
    # 6. ê³µê³µ ESG ê´€ì  ì¢…í•© ì§„ë‹¨  (ì§€ë„ + ê²°ë¡ )
    # --------------------------------------------------------
    with tabs[5]:
        st.subheader("ê³µê³µ ESG ê´€ì ì—ì„œ ë³¸ í‰íƒì‹œ ë…¸ì¸ë³µì§€ì‹œì„¤ ì…ì§€ ì „ëµ")

        # (1) í˜„ì¬ ì¢…í•©ìœ„í—˜ì§€ìˆ˜ / ê²½ê¸°ë„ í‰ê·  / í‰íƒì‹œ PM2.5
        col1, col2, col3 = st.columns(3)
        col1.metric(
            "í‰íƒì‹œ í‰ê·  ì¢…í•©ìœ„í—˜ì ìˆ˜ (1~4)",
            f"{pyeongtaek_risk:.2f}",
        )
        col2.metric(
            "ê²½ê¸°ë„ í‰ê·  ì¢…í•©ìœ„í—˜ì ìˆ˜",
            f"{gyeonggi_mean_risk:.2f}",
            delta=f"{pyeongtaek_risk - gyeonggi_mean_risk:+.2f}",
        )
        col3.metric(
            "í‰íƒì‹œ í‰ê·  PM2.5 (ã/ã¥)",
            f"{pyeongtaek_row['PM25ì¸¡ì •ê°’(ã/ã¥)']:.1f}",
        )

        # ê³µí†µ ë°ì´í„°(ìÂ·ë©´Â·ë™ë³„ ìœ„í—˜ì§€ìˆ˜ + ì¢Œí‘œ)
        has_coords = {"ìœ„ë„", "ê²½ë„"}.issubset(local_risk_map.columns) and not local_risk_map["ìœ„ë„"].isna().all()
        if has_coords:
            base_geo = local_risk_map.dropna(subset=["ìœ„ë„", "ê²½ë„"]).reset_index()
            base_geo = base_geo.rename(
                columns={"í–‰ì •ë™": "ìë©´ë™", "ìœ„ë„": "lat", "ê²½ë„": "lon"}
            )
        else:
            base_geo = None

        # (2) í™˜ê²½ ìœ„í—˜ ì§€ë„ (ë¹¨ê°„ìƒ‰ ì›)
        st.markdown("#### (2) í™˜ê²½ ìœ„í—˜ ì§€ë„ (ìÂ·ë©´Â·ë™ë³„ í™˜ê²½ ìœ„í—˜ì§€ìˆ˜)")
        if has_coords and not base_geo.empty:
            max_risk = float(base_geo["ìœ„í—˜ì§€ìˆ˜"].max())
            min_radius = 300
            max_radius = 1300

            base_geo["marker_radius"] = (
                min_radius
                + (base_geo["ìœ„í—˜ì§€ìˆ˜"] / max_risk) * (max_radius - min_radius)
            )

            env_layer = pdk.Layer(
                "ScatterplotLayer",
                data=base_geo,
                get_position="[lon, lat]",
                get_radius="marker_radius",
                get_fill_color="[255, 0, 0, 140]",  # ë¹¨ê°„ìƒ‰
                pickable=True,
            )

            env_view = pdk.ViewState(
                latitude=float(base_geo["lat"].mean()),
                longitude=float(base_geo["lon"].mean()),
                zoom=10.5,
                pitch=0,
            )

            st.pydeck_chart(
                pdk.Deck(
                    layers=[env_layer],
                    initial_view_state=env_view,
                    tooltip={"text": "ìÂ·ë©´Â·ë™: {ìë©´ë™}\nìœ„í—˜ì§€ìˆ˜: {ìœ„í—˜ì§€ìˆ˜}"},
                )
            )
        else:
            st.info("í™˜ê²½ ìœ„í—˜ ì§€ë„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ìÂ·ë©´Â·ë™ ì¢Œí‘œ(ìœ„ë„/ê²½ë„)ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # (3) ì²­ì • êµ¬ì—­ ì§€ë„ (ì´ˆë¡ìƒ‰ ì›)
        st.markdown("#### (3) ì²­ì • êµ¬ì—­ ì§€ë„ (ìœ„í—˜ì§€ìˆ˜ í•˜ìœ„ ì§€ì—­)")
        if has_coords and not base_geo.empty:
            clean_threshold = base_geo["ìœ„í—˜ì§€ìˆ˜"].quantile(0.30)
            clean_geo = base_geo[base_geo["ìœ„í—˜ì§€ìˆ˜"] <= clean_threshold]

            if not clean_geo.empty:
                clean_layer = pdk.Layer(
                    "ScatterplotLayer",
                    data=clean_geo,
                    get_position="[lon, lat]",
                    get_radius=900,
                    get_fill_color="[0, 200, 0, 180]",  # ì´ˆë¡ìƒ‰
                    pickable=True,
                )

                clean_view = pdk.ViewState(
                    latitude=float(clean_geo["lat"].mean()),
                    longitude=float(clean_geo["lon"].mean()),
                    zoom=10.5,
                    pitch=0,
                )

                st.pydeck_chart(
                    pdk.Deck(
                        layers=[clean_layer],
                        initial_view_state=clean_view,
                        tooltip={"text": "ìÂ·ë©´Â·ë™: {ìë©´ë™}\nìœ„í—˜ì§€ìˆ˜: {ìœ„í—˜ì§€ìˆ˜}"},
                    )
                )
            else:
                st.info("ìœ„í—˜ì§€ìˆ˜ê°€ ë‚®ì€(ì²­ì •) êµ¬ì—­ì´ í†µê³„ì ìœ¼ë¡œ ì¶©ë¶„íˆ ë‚˜ì˜¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì²­ì • êµ¬ì—­ ì§€ë„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ìÂ·ë©´Â·ë™ ì¢Œí‘œ(ìœ„ë„/ê²½ë„)ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # (4) ë…¸ì¸ë³µì§€ì‹œì„¤ ìœ„ì¹˜ ì§€ë„
        st.markdown("#### (4) ë…¸ì¸ë³µì§€ì‹œì„¤ ìœ„ì¹˜ ì§€ë„")
        if {"ìœ„ë„", "ê²½ë„"}.issubset(df_elderly.columns):
            elder_geo = df_elderly.dropna(subset=["ìœ„ë„", "ê²½ë„"]).rename(
                columns={"ìœ„ë„": "lat", "ê²½ë„": "lon"}
            )
            st.map(elder_geo[["lat", "lon"]])
        else:
            st.info(
                "ë…¸ì¸ë³µì§€ì‹œì„¤ ë°ì´í„°ì— ìœ„ë„/ê²½ë„ ì—´ì´ ì—†ìŠµë‹ˆë‹¤. "
                "ë„ë¡œëª…ì£¼ì†Œë¥¼ ì§€ì˜¤ì½”ë”©í•´ 'ìœ„ë„', 'ê²½ë„' ì—´ì„ ì¶”ê°€í•˜ë©´ ì§€ë„ ì‹œê°í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )

        # (5) ê²°ë¡  ì§€ë„: ê´€ë¦¬ ì§‘ì¤‘ / ì‹œì„¤ ì¦ì„¤ ëŒ€ìƒ êµ¬ì—­
        st.markdown("#### (5) ê²°ë¡  ì§€ë„: ë…¸ì¸ë³µì§€ì‹œì„¤ê³¼ í™˜ê²½ ë¦¬ìŠ¤í¬ë¥¼ í•¨ê»˜ ë³¸ ìš°ì„ Â·ì¦ì„¤ ëŒ€ìƒ êµ¬ì—­")
        if has_coords and not base_geo.empty:
            # ìœ„í—˜/ì²­ì • + ì‹œì„¤ ë°€ì§‘/ì·¨ì•½ ê¸°ì¤€ (ë¶„ìœ„ìˆ˜ í™œìš©)
            risk_high_thr = base_geo["ìœ„í—˜ì§€ìˆ˜"].quantile(0.75)
            risk_low_thr = base_geo["ìœ„í—˜ì§€ìˆ˜"].quantile(0.25)
            elder_high_thr = base_geo["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"].quantile(0.75)
            elder_low_thr = base_geo["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"].quantile(0.50)

            # ë…¸ì¸ë³µì§€ì‹œì„¤ ë°€ì§‘ + ìœ„í—˜ ì§€ì—­ â†’ ë³´ë¼ìƒ‰
            focus_geo = base_geo[
                (base_geo["ìœ„í—˜ì§€ìˆ˜"] >= risk_high_thr)
                & (base_geo["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"] >= elder_high_thr)
            ].copy()

            # ë…¸ì¸ë³µì§€ì‹œì„¤ ì·¨ì•½ + ì²­ì • ì§€ì—­ â†’ ì²­ë¡ìƒ‰
            expand_geo = base_geo[
                (base_geo["ìœ„í—˜ì§€ìˆ˜"] <= risk_low_thr)
                & (base_geo["ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜"] <= elder_low_thr)
            ].copy()

            layers = []

            # ì „ì²´ ìÂ·ë©´Â·ë™ì„ ì˜…ì€ íšŒìƒ‰ ì ìœ¼ë¡œ ë°°ê²½ í‘œì‹œ
            layers.append(
                pdk.Layer(
                    "ScatterplotLayer",
                    data=base_geo,
                    get_position="[lon, lat]",
                    get_radius=250,
                    get_fill_color="[120, 120, 120, 60]",
                    pickable=False,
                )
            )

            if not focus_geo.empty:
                focus_layer = pdk.Layer(
                    "ScatterplotLayer",
                    data=focus_geo,
                    get_position="[lon, lat]",
                    get_radius=900,
                    get_fill_color="[160, 0, 200, 200]",  # ë³´ë¼ìƒ‰
                    pickable=True,
                )
                layers.append(focus_layer)

            if not expand_geo.empty:
                expand_layer = pdk.Layer(
                    "ScatterplotLayer",
                    data=expand_geo,
                    get_position="[lon, lat]",
                    get_radius=900,
                    get_fill_color="[0, 190, 190, 200]",  # ì²­ë¡ìƒ‰
                    pickable=True,
                )
                layers.append(expand_layer)

            summary_view = pdk.ViewState(
                latitude=float(base_geo["lat"].mean()),
                longitude=float(base_geo["lon"].mean()),
                zoom=10.5,
                pitch=0,
            )

            st.pydeck_chart(
                pdk.Deck(
                    layers=layers,
                    initial_view_state=summary_view,
                    tooltip={
                        "text": "ìÂ·ë©´Â·ë™: {ìë©´ë™}\n"
                                "ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜: {ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜}ê°œ\n"
                                "ìœ í•´í™”í•™ì‚¬ì—…ì¥ ìˆ˜: {ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜}ê°œ\n"
                                "ìœ„í—˜ì§€ìˆ˜: {ìœ„í—˜ì§€ìˆ˜}"
                    },
                )
            )

            # (6) ê²°ë¡  í…ìŠ¤íŠ¸: í‘œë¡œ ë‚˜íƒ€ë‚´ê¸°
            st.markdown("#### (6) ê²°ë¡  ìš”ì•½ (í‘œ)")
            col_left, col_right = st.columns(2)

            focus_table = focus_geo[["ìë©´ë™", "ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜", "ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜", "ìœ„í—˜ì§€ìˆ˜"]].rename(
                columns={
                    "ìë©´ë™": "ìœ„ì¹˜",
                    "ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜": "í˜„ ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜",
                    "ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜": "í˜„ ìœ í•´í™”í•™ì‚¬ì—…ì¥ ìˆ˜",
                    "ìœ„í—˜ì§€ìˆ˜": "ìœ„í—˜ì§€ìˆ˜ ì¸ë±ìŠ¤",
                }
            )

            expand_table = expand_geo[["ìë©´ë™", "ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜", "ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜", "ìœ„í—˜ì§€ìˆ˜"]].rename(
                columns={
                    "ìë©´ë™": "ìœ„ì¹˜",
                    "ë…¸ì¸ë³µì§€ì‹œì„¤_ìˆ˜": "í˜„ ë…¸ì¸ë³µì§€ì‹œì„¤ ìˆ˜",
                    "ìœ í•´í™”í•™ì‚¬ì—…ì¥_ìˆ˜": "í˜„ ìœ í•´í™”í•™ì‚¬ì—…ì¥ ìˆ˜",
                    "ìœ„í—˜ì§€ìˆ˜": "ìœ„í—˜ì§€ìˆ˜ ì¸ë±ìŠ¤",
                }
            )

            with col_left:
                st.markdown("**ê´€ë¦¬ ì§‘ì¤‘ ëŒ€ìƒ êµ¬ì—­**")
                if focus_table.empty:
                    st.write("ì„ ì •ëœ ê´€ë¦¬ ì§‘ì¤‘ ëŒ€ìƒ êµ¬ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.dataframe(focus_table, use_container_width=True)

            with col_right:
                st.markdown("**ì‹œì„¤ ì¦ì„¤ ëŒ€ìƒ êµ¬ì—­**")
                if expand_table.empty:
                    st.write("ì„ ì •ëœ ì‹œì„¤ ì¦ì„¤ ëŒ€ìƒ êµ¬ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.dataframe(expand_table, use_container_width=True)
        else:
            st.info(
                "ê²°ë¡  ì§€ë„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ìÂ·ë©´Â·ë™ë³„ ì¢Œí‘œ ì •ë³´ê°€ ì—†ì–´ ìš°ì„ /ì¦ì„¤ ëŒ€ìƒ êµ¬ì—­ì„ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )


if __name__ == "__main__":
    main()
